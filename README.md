
![ - a  r  g  e  n  t  o - ](images/argento.png)

## ARGENTO - docker app агент с админкой. 
### Django-приложение для Retrieval-Augmented Generation с использованием Ollama или SDK (Pinecone, ChatGPT).

> [!NOTE]
> Приложение в процессе разработки.


---

Это Django-приложение реализует возможность легкого развертывания чат бота с RAG (Retrieval-Augmented Generation) для выполнения бизнес задач. 

#### Сценарий использования:
- Через админ панель загружается документация (текст или PDF).
- Документация переобразуется в векрторы для RAG (Vector Storage на борту) или SDK например Pinecone.
- При необходимости при помощи утилиты `Django Commands` запускаем телеграм бот TelegramBotAPI. Бот диаложит в контексте бизнес задачи. Инструктирует, консультирует, вызывает function tools...
- Создаем ендпоинт для работы с конкретной тематикой\топиком на случай если нужен подобный микросервис например для веб сайта.
- Ендпоинт возвращает сгенерированные ответы моделью в обученом контексте.
- Есть возможность настроить Function Tools который может выполнять задачи (например высылать документы, делать заказы, формировать подборки товаров).
- В админке есть выгрузка таблиц по клиентам\диалогам и статистика.
* Тут описаны основные функции. Так как сервис в процессе разработки, возможно что то добавится или изменится.


#### Основные компоненты:

- Ollama — локально развёрнутая LLM-модель, доступная через API.
PDF-документы и текстовые записи:
- PDF-файлы загружаются через админку Django и автоматически индексируются.
Обычный текст можно добавлять через форму на админке.
Скрипт для запуска Телеграм бота. 
Векторное хранилище — тексты и PDF преобразуются в эмбеддинги и сохраняются.
RAG-интерфейс — на основе пользовательского запроса извлекаются релевантные ответы, которые используются как контекст.
Панель администратора — позволяет управлять PDF-файлами и выгружать хронологию запросов.


#### Приемущества сервиса.

- Быстрый старт, загрузка документации из админ панели которая переобразовывается в эмбединги.
- RAG по нескольком топикам, или несколько векторного хранилища. Масштабируемость задач.
- Function calling - вызывает методы API к которым привязаны функции или задачи. (генерация файлов, выборка данных, поиск, что угодно).
- В query params передаем данные на REST API >> приложение возвращает результат.
- Получение данных о пользователе в query параметрах url, на случай авторизации с фронтенд или других источников ввода данных.
- Инициация общения с ботом (бот для среды разработки @Test_Assistant_bot )
- Оповещение в группу (@testassistantbotchannel) или канал - Пользователь, `КАКОЕ ТО ИМЯ` хочет чтоб ему перезвонили на номер: `+79....0000123`
- Общение с виртуальным ассистентом Pinecon / OpenAI / LangChain / Ollama и etc.
- Админ панель с выгрузкой данных в формате XLSX (ну пока есть но не знаю...)
- ** Опции будут дополнены по мере разработки.


### Запуск

- клонируем репозиторий `git@github.com:ivanIStereotekk/bot-station.git`
- переименовывваем файл с переменными окружения из ` .env_example ` в ` .env `
- запускаем в директории с проектом ` docker compose up `

---
#### Для корректоной работы OpenAI ассистента необходим VPN или сервис должен быть запущен не в РФ.
---

####  c u r r e n t  состояние:

- скрипт entrypoint.sh автоматически создает superuser, миграции, и запускает бота.

- на сайте администратора имеется возможность импорта/экспорта exel файлов с данными  

- по группам, по фильтрам, по датам, все вместе...

- данные для входа в админку в .env файле `DJANGO_SUPERUSER_NAME` `DJANGO_SUPERUSER_PASS`

- OpenAI SDK - генерация текста на базе чатов/ботов в Telegram

- Faiss vector store


---



#### URL с параметрами для `POST` запроса

> [!NOTICE]
> URL должен соответствовать вашему домену или имени хоста в docker.

 ```http://localhost:8000/send_user_query_data/?name=Jichael%20Mackson&email=juchael.mackson%40gmail.com&phone=%+79883442299&comment=Jichael_Mackson%&topic=Helloworld&contact=Мои&Контакты```




## Если хотим запуститься локально DEBUG (Local Setup):

Создаем в рабочей директории .env файл (референс .env_example)

- Устанавливаем при помощи Pyenv версию интерпритатора (Python 3.11) для проекта.
- Создаем окружение `pyenv virtualenv 3.11 argento_venv`
- Активируем окружение `pyenv activate argento_venv`
- Устанвливаем зависимости `pip install -r requirements.txt`
- Создаем миграции `python manage.py makemigrations`
- Пишем миграции в БД `python manage.py migrate`
- Создаем автоматически (Django Commands) суперюзера для доступа к админке `python manage.py superuser_create` (Не забыть указать нужные параметры в файле .env)
- Или можем создать суперюзера стандартным методом `python manage.py createsuperuser` отвечаем на вопросы...

<!-- Можем создать скрипт sh с теми же командами:

```bash
#!/bin/sh

pyenv virtualenv 3.11 argento_venv

pyenv activate argento_venv

pip install -r requirements.txt

cd ./botlog

python manage.py makemigrations --no-input

python manage.py migrate --no-input

python manage.py collectstatic --no-input

python manage.py superuser_create


python manage.py runserver 0.0.0.0:8000 & python manage.py bot_notify

# exit

``` -->


---

## Установка Ollama:

> [!NOTE]
> Ollama is the easiest way to get up and running with large language models such as gpt-oss, Gemma 3, DeepSeek-R1, Qwen3 and more.
> Ollama guide Linux- https://docs.ollama.com/linux



- Устанавливаем - `curl -fsSL https://ollama.com/install.sh | sh`

- Запускаем модель (например Llama3.2) - `ollama run llama3.2`


